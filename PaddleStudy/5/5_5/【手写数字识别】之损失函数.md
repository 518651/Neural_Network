# 【手写数字识别】之损失函数



# 概述

损失函数是模型优化的目标，用于在众多的参数取值中，识别最理想的取值。损失函数的计算在训练过程的代码中，每一轮模型训练的过程都相同，分如下三步：

1.根据输入数据正向计算预测输出

2.再根据预测值和真实值计算损失

3.根据损失反向传播梯度并更新参数





# 分类任务的损失函数

**不同的深度学习任务需要有各自适宜的损失函数**

我们以房价预测和手写数字识别两个任务为例，详细剖析其中的缘由如下：

1.房价预测是回归任务，而手写数字识别是分类任务，

2.房价可以是大于0的任何浮点数，而手写数字识别的输出可能是0~9之间的10个整数，相当于一种标签

3.在房价预测案例中，由于房价本身是一个连续的实数值，因此以模型输出的数值和真实房价差距作为损失函数（LOSS）是符合道理。但对应分类问题，真实的结果应该是分类标签，而模型输出是实数值，导致以两者相减作为损失不具备物理意义



那么，什么是分类任务的合理输出呢？分类任务本质上是“某种特征组合下的分类概率”，下面以一个简单案例说明，如 **图2** 所示。

![img](https://ai-studio-static-online.cdn.bcebos.com/c9f479c2960140839b259ca7ab2256a0dcd7a714e76a4edfb5377f1566796460)


图2：观测数据和背后规律之间的关系

在本案例中，医生根据肿瘤大小x作为肿瘤性质y*的参考判断（判断的因素有很多，肿瘤大小只是其中之一），那么我们观测到该模型判断的结果是x和*y的标签（1为恶性，0为良性）。而这个数据背后的规律是不同大小的肿瘤，属于恶性肿瘤的概率。观测数据是真实规律抽样下的结果，分类模型应该拟合这个真实规律，输出属于该分类标签的概率。



## Softmax函数

如果模型能输出10个标签的概率，对应真实标签的概率输出尽可能接近100%，而其他标签的概率输出尽可能接近0%，且所有输出概率之和为1。这是一种更合理的假设！与此对应，真实的标签值可以转变成一个10维度的one-hot向量，在对应数字的位置上为1，其余位置为0，比如标签“6”可以转变成[0,0,0,0,0,0,1,0,0,0]。

为了实现上述思路，需要引入Softmax函数，它可以将原始输出转变成对应标签的概率，公式如下，其中*C*是标签类别个数。



每个输出的范围均在0~1之间，且所有输出之和等于1，这是这种变换后可被解释成概率的基本前提。对应到代码上，需要在前向计算中，对全连接网络的输出层增加一个Softmax运算，`outputs = F.softmax(outputs)`。

**图3** 是一个三个标签的分类模型（三分类）使用的Softmax输出层，从中可见原始输出的三个数字3、1、-3，经过Softmax层后转变成加和为1的三个概率值0.88、0.12、0。

![img](https://ai-studio-static-online.cdn.bcebos.com/ef129caf64254318821e9410bb71ab1f45fff20e4282482986081d44a1e3bcbb)

上文解释了为何让分类模型的输出拟合概率的原因，但为何偏偏用Softmax函数完成这个职能？ 下面以二分类问题（只输出两个标签）进行原理的探讨。

对于二分类问题，使用两个输出接入Softmax作为输出层，等价于使用单一输出接入Sigmoid函数。如 **图4** 所示，利用两个标签的输出概率之和为1的条件，Softmax输出0.6和0.4两个标签概率，从数学上等价于输出一个标签的概率0.6。

![img](https://ai-studio-static-online.cdn.bcebos.com/4dbdf378438f42b0bc6de6f11955834b7063cc6916544017b0af2ccf1f730984)


图4：对于二分类问题，等价于单一输出接入Sigmoid函数



在这种情况下，只有一层的模型为
$$

$$
为Sigmoid函数。模型预测为1的概率为S(wTxi)S(w^{T}x_i)*S*(*w**T**x**i*)，模型预测为0的概率为1−S(wTxi)1-S(w^{T}x_i)1−*S*(*w**T**x**i*)。

**图5** 是肿瘤大小和肿瘤性质的数据图。从图中可发现，往往尺寸越大的肿瘤几乎全部是恶性，尺寸极小的肿瘤几乎全部是良性。只有在中间区域，肿瘤的恶性概率会从0逐渐到1（绿色区域），这种数据的分布是符合多数现实问题的规律。如果我们直接线性拟合，相当于红色的直线，会发现直线的纵轴0-1的区域会拉的很长，而我们期望拟合曲线0-1的区域与真实的分类边界区域重合。那么，观察下Sigmoid的曲线趋势可以满足我们对个问题的一切期望，它的概率变化会集中在一个边界区域，有助于模型提升边界区域的分辨率。

![img](https://ai-studio-static-online.cdn.bcebos.com/bbf5e0eda62c44bb84528dbfd8642ef901b2dc42c6f541bc8cd0b75b967dc934)


图5：使用Sigmoid拟合输出可提高分类模型对边界的分辨率



## Softmax函数

如果模型能输出10个标签的概率，对应真实标签的概率输出尽可能接近100%，而其他标签的概率输出尽可能接近0%，且所有输出概率之和为1.这是一种更合理的假设！与此对应，真实的标签值可以转变成10维度的one-hot向量，在对应数字的位置上为1，其他位置为0，**比如标签”6“可以转变成[0,0,0,0,0,0,1,0,0,0]**

为了实现上述思路，需要引入Softmax函数，它可以将原始输出转变对应标签概率，公式如下，其中C是标签类别个数
$$
softmax(X_i)= \frac{e^{xi}}{\sum_{j=0}^N{e^xj}},i=0....,C-1
$$
从公式可以体现，每个输出的范围均在0~1之间，且所有输出之和等于1，这是这种变换后可被解释成概率的基本前提。对应到代码上，需要在前向计算中，对全连接网络的输出层增加一个Softmax运算，outputs= F.softmax(outputs).

图3 是一个三个标签的分类模型（三分类）使用的Softmax输出层，从中可见原始输出的三个数字3，-，-3，经过Softmax层后转变加和1的三个概率值0.88,0.12,0.

![img](https://ai-studio-static-online.cdn.bcebos.com/ef129caf64254318821e9410bb71ab1f45fff20e4282482986081d44a1e3bcbb)
	图3：网络输出层改为Softmax函数

上文解释了为何让分类模型的输出拟合概率的原因，但为何偏偏用Softmax函数完成这个职能？ 下面以二分类问题（只输出两个标签）进行原理的探讨。

对于二分类问题，使用两个输出接入Softmax作为输出层，等价于使用单一输出接入Sigmoid函数。如 **图4** 所示，利用两个标签的输出概率之和为1的条件，Softmax输出0.6和0.4两个标签概率，从数学上等价于输出一个标签的概率0.6。

